<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niko Sünderhauf | Bayesian Deep Learning and Uncertainty in Object Detection</title>
  <meta name="description" content="">

  <link rel="shortcut icon" href="https://nikosuenderhauf.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/main.css">
  <link rel="canonical" href="https://nikosuenderhauf.github.io/projects/uncertainty/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Niko</strong> Sünderhauf
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://nikosuenderhauf.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/cv/">bio</a>
          
        
          
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/projects/">research</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/jobs/">recruiting</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/workshops/">workshops</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Bayesian Deep Learning and Uncertainty in Object Detection</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Bayesian Deep Learning and Uncertainty in Object Detection clearfix">
    <p>In order to fully integrate deep
learning into robotics, it is important that deep learning systems
can reliably estimate the uncertainty in their predictions.
This would allow robots to treat a deep neural network
like any other sensor, and use the established Bayesian
techniques to fuse the network’s predictions
with prior knowledge or other sensor measurements, or to
accumulate information over time.</p>

<p>Deep learning systems, e.g. for classification or detection, typically return scores
from their softmax layers that are proportional to the system’s
confidence, but are not calibrated probabilities, and therefore
not useable in a Bayesian sensor fusion framework.</p>

<p>Current approaches towards uncertainty estimation for
deep learning are calibration techniques, or
Bayesian deep learning with approximations such
as Monte Carlo Dropout or ensemble methods.</p>

<p>Our work focusses on Bayesian Deep Learning approaches for the specific use case of object detection on a robot in open-set conditions.</p>

<h3 id="publications">Publications</h3>

<ol class="bibliography"><li>

<div id="miller2020cac">
  
    
    <a href="https://arxiv.org/abs/2004.02434" class="title" target="_blank">Class Anchor Clustering: A Loss for Distance-based Open Set Recognition</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Winter Conference on Applications of Computer Vision (WACV),</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2004.02434" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller2020cac.png" /></a>
  
  
  <span class="abstract">
    Existing open set classifiers distinguish between known and unknown inputs by measuring distance in a network’s logit space, assuming that known inputs cluster closer to the training data than unknown inputs. However, this approach is typically applied post-hoc to networks trained with cross-entropy loss, which neither guarantees nor encourages the hoped-for clustering behaviour. To overcome this limitation, we introduce Class Anchor Clustering (CAC) loss. CAC is an entirely distance-based loss that explicitly encourages training data to form tight clusters techniques on the challenging TinyImageNet dataset, achieving a 2.4% performance increase in AUROC. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2004.02434" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2004.02434" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rahman2020performance">
  
    
    <a href="https://arxiv.org/abs/2009.08650" class="title" target="_blank">Performance Monitoring of Object Detection During Deployment</a>
    
    <span class="author">
      
        
          
            
              Quazi Marufur Rahman,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2009.08650,</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2009.08650" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rahman2020performance.png" /></a>
  
  
  <span class="abstract">
    Performance monitoring of object detection is crucial for safety-critical applications such as autonomous vehicles that operate under varying and complex environmental conditions. Currently, object detectors are evaluated using summary metrics based on a single dataset that is assumed to be representative of all future deployment conditions. In practice, this assumption does not hold, and the performance fluctuates as a function of the deployment conditions. To address this issue, we propose an introspection approach to performance monitoring during deployment without the need for ground truth data. We do so by predicting when the per-frame mean average precision drops below a critical threshold using the detector’s internal features.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2009.08650" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2009.08650" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="hall2018probability">
  
    
    <a href="https://arxiv.org/abs/1811.10800" class="title" target="_blank">Probabilistic Object Detection: Definition and Evaluation</a>
    
    <span class="author">
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              John Skinner,
            
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        
          
            
              Gustavo Carneiro,
            
          
        
      
        
          
            
              Anelia Angelova,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Winter Conference on Applications of Computer Vision (WACV),</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1811.10800" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/hall2018probability.png" /></a>
  
  
  <span class="abstract">
    We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections. Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ). Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1811.10800" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1811.10800" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="suenderhauf2019probabilistic">
  
    
    <a href="https://rdcu.be/bQR84" class="title" target="_blank">A Probabilistic Challenge for Object Detection</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              John Skinner,
            
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Gustavo Carneiro,
            
          
        
      
        

          
            
              Peter Corke.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Nature Machine Intelligence,</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://rdcu.be/bQR84" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf19probabilistic.png" /></a>
  
  
  <span class="abstract">
    To safely operate in the real world, robots need to evaluate how confident they are about what they see.
  A new competition challenges computer vision algorithms to not just detect and localize objects, but also report how certain they are.
  To this end, we introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://rdcu.be/bQR84" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="miller2019benchmarking">
  
    
    <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf" class="title" target="_blank">Benchmarking Sampling-based Probabilistic Object Detectors</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              David Hall,
            
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller2019benchmarking.png" /></a>
  
  
  <span class="abstract">
    This paper provides the first benchmark for sampling-based probabilistic object detectors. A probabilistic object
  detector expresses uncertainty for all detections that reliably indicates object localisation and classification performance. We compare performance for two sampling-based
  uncertainty techniques, namely Monte Carlo Dropout and Deep Ensembles, when implemented into one-stage and
  two-stage object detectors, Single Shot MultiBox Detector and Faster R-CNN.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Miller19sampling">
  
    
    <a href="https://arxiv.org/abs/1809.06006" class="title" target="_blank">Evaluating Merging Strategies for Sampling-based Uncertainty Techniques in Object Detection</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1809.06006" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller18sampling.png" /></a>
  
  
  <span class="abstract">
    There has been a recent emergence of sampling-based techniques for estimating epistemic uncertainty in deep neural networks. While these methods can be applied to classification or semantic segmentation tasks by simply averaging samples, this is not the case for object detection, where detection sample bounding boxes must be accurately associated and merged. A weak merging strategy can significantly degrade the performance of the detector and yield an unreliable uncertainty measure. This paper provides the first in-depth investigation of the effect of different association and merging strategies. We compare different combinations of three spatial and two semantic affinity measures with four clustering methods for MC Dropout with a Single Shot Multi-Box Detector. Our results show that the correct choice of affinity-clustering combinations can greatly improve the effectiveness of the classification and spatial uncertainty estimation and the resulting object detection performance. We base our evaluation on a new mix of datasets that emulate near open-set conditions (semantically similar unknown classes), distant open-set conditions (semantically dissimilar unknown classes) and the common closed-set conditions (only known classes).
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1809.06006" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1809.06006" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rahman2019traffic">
  
    
    <a href="https://arxiv.org/abs/1903.06391" class="title" target="_blank">Did You Miss the Sign? A False Negative Alarm System for Traffic Sign Detectors</a>
    
    <span class="author">
      
        
          
            
              Quazi Marufur Rahman,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1903.06391" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rahman2019traffic.png" /></a>
  
  
  <span class="abstract">
    In this paper, we propose an approach to identify traffic signs that have been mistakenly discarded by the object detector. The proposed method raises an alarm when it discovers a failure by the object detector to detect a traffic sign. This approach can be useful to evaluate the performance of the detector during the deployment phase. We trained a single shot multi-box object detector to detect traffic signs and used its internal features to train a separate false negative detector (FND). During deployment, FND decides whether the traffic sign detector has missed a sign or not.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1903.06391" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1903.06391" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Miller18">
  
    
    <a href="http://arxiv.org/abs/1710.06677" class="title" target="_blank">Dropout Sampling for Robust Object Detection in Open-Set Conditions</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/1710.06677" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller18dropout.png" /></a>
  
  
  <span class="abstract">
    Dropout Variational Inference, or Dropout Sampling, has been recently proposed as an
approximation technique for Bayesian Deep Learning and evaluated for image classification
and regression tasks. This paper investigates the utility of Dropout Sampling for object
detection for the first time. We demonstrate how label uncertainty can be extracted from a
state-of-the-art object detection system via Dropout Sampling. We show that this uncertainty
can be utilized to increase object detection performance under the open-set conditions that
are typically encountered in robotic vision. We evaluate this approach on a large synthetic
dataset with 30,000 images, and a real-world dataset captured by a mobile robot in a
versatile campus environment.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/1710.06677" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Miller17a">
  
    
    <a href="http://bayesiandeeplearning.org/2017/papers/20.pdf" class="title" target="_blank">Dropout Variational Inference Improves Object Detection in Open-Set Conditions</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of NIPS Workshop on Bayesian Deep Learning,</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="http://bayesiandeeplearning.org/2017/papers/20.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller17dropout.png" /></a>
  
  
  <span class="abstract">
    One of the biggest current challenges of visual object detection is reliable operation in open-set
conditions. One way to handle the open-set problem is to utilize the uncertainty of the model to reject predictions
with low probability. Bayesian Neural Networks (BNNs), with variational inference commonly
used as an approximation, is an established approach to estimate model uncertainty. Here we extend the concept of Dropout sampling to object detection for the first time. We evaluate
Bayesian object detection on a large synthetic and a real-world dataset and show how the estimated
label uncertainty can be utilized to increase object detection performance under open-set conditions.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://bayesiandeeplearning.org/2017/papers/20.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Dayoub17">
  
    
    <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf" class="title" target="_blank">Episode-Based Active Learning with Bayesian Neural Networks</a>
    
    <span class="author">
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Corke.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Deep Learning for Robotic Vision, Conference on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/dayoub17active.png" /></a>
  
  
  <span class="abstract">
    We investigate different strategies for active learning
with Bayesian deep neural networks. We focus our analysis
on scenarios where new, unlabeled data is obtained episodically,
such as commonly encountered in mobile robotics
applications. An evaluation of different strategies for acquisition,
updating, and final training on the CIFAR-10 dataset
shows that incremental network updates with final training
on the accumulated acquisition set are essential for best
performance, while limiting the amount of required human
labeling labor.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2020 Niko Sünderhauf.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://nikosuenderhauf.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://nikosuenderhauf.github.io/assets/js/katex.js"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>






<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135749210-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
