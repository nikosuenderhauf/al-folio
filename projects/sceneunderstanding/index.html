<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niko Sünderhauf | Scene Understanding and Semantic SLAM</title>
  <meta name="description" content="">

  <link rel="shortcut icon" href="https://nikosuenderhauf.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/main.css">
  <link rel="canonical" href="https://nikosuenderhauf.github.io/projects/sceneunderstanding/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Niko</strong> Sünderhauf
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://nikosuenderhauf.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/cv/">bio</a>
          
        
          
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/projects/">research</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/jobs/">recruiting</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/workshops/">workshops</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Scene Understanding and Semantic SLAM</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Scene Understanding and Semantic SLAM clearfix">
    <p>Making a robot understand what it sees is one of the most fascinating goals in my current research.
To this end, we develop novel methods for <em>Semantic Mapping</em> and <a href="http://www.semanticslam.ai">Semantic SLAM</a> by combining object detection with simultaneous localisation and mapping (SLAM) techniques. We furthermore work on <a href="uncertainty">Bayesian Deep Learning</a> for object detection, to better understand the uncertainty of a deep network’s predictions and integrate deep learning into robotics in a probabilistic way.</p>

<!-- The problem of Simultaneous Localization and Mapping (SLAM) describes the process of a robot building a map of its unknown environment, and at the same time using this still incomplete map to determine the robot’s position, and to navigate.

SLAM is not unlike what seafarers in the past had to do when they explored the coast of a new continent for the first time.

Most current SLAM systems are still based on primitive geometric features such as points, lines, or planes. The created maps therefore carry geometric information, but no immediate semantic information. For instance in the image below, we see a map consisting of many individual points.

For us humans it is quite easy to identify individual objects such as monitors or chairs in this point cloud map. We automatically connect meaning (semantics) to the geometric structure we see. For a robot however, interpreting the map in this semantic way is a very hard problem.

A robot that uses this point cloud map – for instance for navigation – can understand that something is in its way, but it does not know what kind of object it is: which of these many points are part of a chair? Which represent a monitor? Which belong to a human office worker?

**Semantic Mapping** enriches the geometric map by semantic information. We can see below how some points in the map got identified as belonging to an object of a certain type. We illustrate this by assigning different colors to different object types, e.g. light blue for monitors and dark blue for keyboards.


**Semantic SLAM** goes one step further. Semantic SLAM uses objects as the central entities in the map (instead of primitives such as points). The objects carry semantic meaning, such as class labels or affordances. This -->

<h3 id="semantic-slam-and-semantic-mapping">Semantic SLAM and Semantic Mapping</h3>
<p>We work on novel approaches to SLAM (Simultaneous Localization and Mapping) that create semantically meaningful maps by combining geometric and semantic information.</p>

<p>We believe such semantically enriched maps will help robots understand our complex world and will ultimately increase the range and sophistication of interactions that robots can have in domestic and industrial deployment scenarios.</p>

<p>Read more on our dedicated project website <a href="http://www.semanticslam.ai">semanticslam.ai</a>.</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/w1-INFCpc20" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
</center>

<ol class="bibliography"><li>

<div id="nicholson18quadricslam">
  
    
    <a href="https://ieeexplore.ieee.org/document/8440105/" class="title" target="_blank">QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Object-oriented SLAM</a>
    
    <span class="author">
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters (RA-L),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/document/8440105/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/nicholson18quadricslam.png" /></a>
  
  
  <span class="abstract">
    In this paper, we use 2D object detections from multiple views to simultaneously estimate a 3D quadric surface for each object and localize the camera position. We derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D object detections can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for object detectors that addresses the challenge of partially visible objects, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1804.04011" target="_blank">arXiv</a>]
  
  
    [<a href="http://semanticslam.ai/quadricslam.html" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/document/8440105/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="hosseinzadeh2018structure">
  
    
    <span class="title">Structure Aware SLAM using Quadrics and Planes</span>
    
    <span class="author">
      
        
          
            
              M Hosseinzadeh,
            
          
        
      
        
          
            
              Y Latif,
            
          
        
      
        
          
            
              T Pham,
            
          
        
      
        
          
            
              N Sünderhauf,
            
          
        
      
        

          
            
              I Reid.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of Asian Conference on Computer Vision (ACCV),</em>
    
    
      2018.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
    [<a href="http://arxiv.org/abs/1804.09111" target="_blank">arXiv</a>]
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Jablonsky18geometric">
  
    
    <a href="https://arxiv.org/abs/1809.06977" class="title" target="_blank">An Orientation Factor for Object-Oriented SLAM</a>
    
    <span class="author">
      
        
          
            
              Natalie Jablonsky,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint,</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1809.06977" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/jablonsky18geometric.png" /></a>
  
  
  <span class="abstract">
    Current approaches to object-oriented SLAM lack the ability to incorporate prior knowledge of the scene geometry, such as the expected global orientation of objects. We overcome this limitation by proposing a geometric factor that constrains the global orientation of objects in the map, depending on the objects’ semantics. This new geometric factor is a first example of how semantics can inform and improve geometry in object-oriented SLAM. We implement the geometric factor for the recently proposed QuadricSLAM that represents landmarks as dual quadrics. The factor probabilistically models the quadrics’ major axes to be either perpendicular to or aligned with the direction of gravity, depending on their semantic class. Our experiments on simulated and real-world datasets show that using the proposed factors to incorporate prior knowledge improves both the trajectory and landmark quality.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1809.06977" target="_blank">arXiv</a>]
  
  
    [<a href="http://semanticslam.ai/geometricfactors.html" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1809.06977" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="nicholson18quadricslam">
  
    
    <a href="https://arxiv.org/abs/1804.04011v1" class="title" target="_blank">QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Semantic SLAM</a>
    
    <span class="author">
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Representing a Complex World, International Conference on Robotics and Automation (ICRA),</em>
    
    
      2018.
    
    </span>
  

  
  <span class="award">Best Workshop Paper Award</span>
  

  
  <a href="https://arxiv.org/abs/1804.04011v1" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/nicholson18quadricslamworkshop.png" /></a>
  
  
  <span class="abstract">
    We derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D bounding boxes (such as those typically obtained from visual object detection systems) can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for deep-learned object detectors that addresses the challenge of partial object detections often encountered in robotics applications, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1804.04011" target="_blank">arXiv</a>]
  
  
    [<a href="http://semanticslam.ai/quadricslam.html" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1804.04011v1" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Suenderhauf17a">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/8206392/" class="title" target="_blank">Meaningful Maps With Object-Oriented Semantic Mapping</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Trung T. Pham Pham,
            
          
        
      
        
          
            
              Yasir Latif,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/8206392/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf17maps.png" /></a>
  
  
  <span class="abstract">
    For intelligent robots to interact in meaningful ways with their environment, they must understand both the geometric and semantic properties of the scene surrounding them. The majority of research to date has addressed these mapping challenges separately, focusing on either geometric or semantic mapping. In this paper we address the problem of building environmental maps that include both semantically meaningful, object-level entities and point- or mesh-based geometrical representations. We simultaneously build geometric point cloud models of previously unseen instances of known object classes and create a map that contains these object models as central entities. Our system leverages sparse, feature-based RGB-D SLAM, image-based deep-learning object detection and 3D unsupervised segmentation.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/8206392/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Suenderhauf16">
  
    
    <a href="http://ieeexplore.ieee.org/abstract/document/7487796/" class="title" target="_blank">Place Categorization and Semantic Mapping on a Mobile Robot</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Ruth Schulz,
            
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        
          
            
              Gordon Wyeth,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2016.
    
    </span>
  

  

  
  <a href="http://ieeexplore.ieee.org/abstract/document/7487796/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf16mapping.png" /></a>
  
  
  <span class="abstract">
    In this paper we focus on the challenging problem of place categorization and semantic mapping on a robot without environment-specific training. Motivated by their ongoing success in various visual recognition tasks, we build our system upon a state-of-the-art convolutional network. We overcome its closed-set limitations by complementing the network with a series of one-vs-all classifiers that can learn to recognize new semantic classes online. Prior domain knowledge is incorporated by embedding the classification system into a Bayesian filter framework that also ensures temporal coherence. We evaluate the classification accuracy of the system on a robot that maps a variety of places on our campus in real-time. We show how semantic information can boost robotic object detection performance and how the semantic map can be used to modulate the robot’s behaviour during navigation tasks. The system is made available to the community as a ROS module.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://ieeexplore.ieee.org/abstract/document/7487796/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Suenderhauf15d">
  
    
    <a href="https://eprints.qut.edu.au/109668/1/109668.pdf" class="title" target="_blank">SLAM – Quo Vadis? In Support of Object Oriented and Semantic SLAM</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            
              Markus Eich,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on The Problem of Moving Sensors, Robotics: Science and Systems (RSS),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="https://eprints.qut.edu.au/109668/1/109668.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf15d.png" /></a>
  
  
  <span class="abstract">
    Most current SLAM systems are still based on
primitive geometric features such as points, lines, or planes.
The created maps therefore carry geometric information, but
no immediate semantic information. With the recent significant
advances in object detection and scene classification we think the
time is right for the SLAM community to ask where the SLAM
research should be going during the next years. As a possible
answer to this question, we advocate developing SLAM systems
that are more object oriented and more semantically enriched
than the current state of the art. This paper provides an overview
of our ongoing work in this direction.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://eprints.qut.edu.au/109668/1/109668.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<hr />

<h3 id="scene-understanding">Scene Understanding</h3>
<ol class="bibliography"><li>

<div id="Trung18">
  
    
    <a href="http://arxiv.org/abs/1709.07158" class="title" target="_blank">SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes</a>
    
    <span class="author">
      
        
          
            
              Trung T. Pham,
            
          
        
      
        
          
            
              Thanh-Toan Do,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/1709.07158" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/pham18scenecut.png" /></a>
  
  
  <span class="abstract">
    This paper presents SceneCut, a novel approach to jointly discover previously unseen
objects and non-object surfaces using a single RGB-D image. SceneCut’s joint reasoning
over scene semantics and geometry allows a robot to detect and segment object instances
in complex scenes where modern deep learning-based methods either fail to separate
object instances, or fail to detect objects that were not seen during training. SceneCut
automatically decomposes a scene into meaningful regions which either represent objects
or scene surfaces. The decomposition is qualified by an unified energy function over
objectness and geometric fitting. We show how this energy function can be optimized
efficiently by utilizing hierarchical segmentation trees.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/1709.07158" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Rezazadegan15">
  
    
    <span class="title">Enhancing Human Action Recognition with Region Proposals</span>
    
    <span class="author">
      
        
          
            
              Fahimeh Rezazadegan,
            
          
        
      
        
          
            
              Sareh Shirazi,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2015.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Suenderhauf15b">
  
    
    <a href="https://eprints.qut.edu.au/109683/1/109683.pdf" class="title" target="_blank">Continuous Factor Graphs For Holistic Scene Understanding</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Scene Understanding (SUNw), Intl. Conf. on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="https://eprints.qut.edu.au/109683/1/109683.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf15b.png" /></a>
  
  
  <span class="abstract">
    We propose a novel mathematical formulation for the
holistic scene understanding problem and transform it from
the discrete into the continuous domain. The problem can
then be modeled with a nonlinear continuous factor graph,
and the MAP solution is found via least squares optimization.
We evaluate our method on the realistic NYU2 dataset.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://eprints.qut.edu.au/109683/1/109683.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<hr />

<h3 id="scene-understanding-hazard-detection">Scene Understanding: Hazard Detection</h3>
<ol class="bibliography"><li>

<div id="McMahon17">
  
    
    <a href="https://ieeexplore.ieee.org/document/7959072/" class="title" target="_blank">Multi-Modal Trip Hazard Affordance Detection On Construction Sites</a>
    
    <span class="author">
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael J Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters (RA-L),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/document/7959072/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/mcmahon17hazards.png" /></a>
  
  
  <span class="abstract">
    Trip hazards are a significant contributor to accidents on construction and manufacturing sites. We conduct a comprehensive investigation into the performance characteristics of 11 different colors and depth fusion approaches, including four fusion and one nonfusion approach, using color and two types of depth images. Trained and tested on more than 600 labeled trip hazards over four floors and 2000 m2 in an active construction site, this approach was able to differentiate between identical objects in different physical configurations. Outperforming a color-only detector, our multimodal trip detector fuses color and depth information to achieve a 4% absolute improvement in F1-score. These investigative results and the extensive publicly available dataset move us one step closer to assistive or fully automated safety inspection systems on construction sites.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/document/7959072/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="McMahon17a">
  
    
    <a href="http://www.araa.asn.au/acra/acra2017/papers/pap104s1-file1.pdf" class="title" target="_blank">Auxiliary Tasks to Improve Trip Hazard Affordance Detection</a>
    
    <span class="author">
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            
              Tong Shen,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        
          
            
              Chunhua Shen,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="http://www.araa.asn.au/acra/acra2017/papers/pap104s1-file1.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/mcmahon17a.png" /></a>
  
  
  <span class="abstract">
    We propose to train a CNN performing pixel-wise trip detection with three auxiliary tasks to help the CNN better infer scene geometric properties of trip hazards. Of the three approaches investigated pixel-wise ground plane estimation, pixel depth estimation and pixel height above ground plane estimation, the first approach allowed the trip detector to achieve a 11.1% increase in Trip IOU over earlier work. These new approaches make it plausible to deploy a robotic platform to perform trip hazard detection, and so potentially reduce the number of injuries on construction sites.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://www.araa.asn.au/acra/acra2017/papers/pap104s1-file1.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="McMahon15b">
  
    
    <a href="http://www.araa.asn.au/acra/acra2015/papers/pap158.pdf" class="title" target="_blank">TripNet: Detecting Trip Hazards on Construction Sites</a>
    
    <span class="author">
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="http://www.araa.asn.au/acra/acra2015/papers/pap158.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/mcmahon15.png" /></a>
  
  
  <span class="abstract">
    This paper introduces TripNet, a robotic vision system that detects trip hazards using raw construction site images.
  TripNet performs trip hazard identification using only camera imagery and minimal training with a pre-trained Convolutional Neural Network (CNN) rapidly fine-tuned on a small corpus of labelled image regions from construction sites. There is no reliance on prior scene segmentation methods during deployment. Trip-Net achieves comparable performance to a human on a dataset recorded in two distinct real world construction sites. TripNet exhibits spatial and temporal generalization by functioning in previously unseen parts of a construction site and over time periods of several weeks.
  
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://www.araa.asn.au/acra/acra2015/papers/pap158.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2020 Niko Sünderhauf.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://nikosuenderhauf.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://nikosuenderhauf.github.io/assets/js/katex.js"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>






<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135749210-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
